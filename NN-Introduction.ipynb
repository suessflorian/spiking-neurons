{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef5ec409-ac7a-4ffe-81f8-6f0c68f3f2a4",
   "metadata": {},
   "source": [
    "# MNIST Database\n",
    "\n",
    "The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. Widely used for training and testing in the field of machine learning.\n",
    "\n",
    "- images of upper & lower case letters plus digits\n",
    "- 28x28 pixel\n",
    "- grayscale\n",
    "- ~70,000 labelled images.\n",
    "\n",
    "## Loading this via `torchvision`\n",
    "\n",
    "A pytorch extension that consists of popular datasets, yet critically baking into them the separation of training and testing datasets to enforce a bit more consistency across research, as everyone uses the same split - allows for comparable benchmarking of models.\n",
    "\n",
    "`torchvision` allows you to \"transform\" your data as you load it in, which is typically required as you interface your dataset with your NN. We will explicitly avoid the use of this utility to describe nessecary transformations explicitly later. I'm still learning after all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "570ba213-d7f0-4f25-af5f-cca99e0a3580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True)\n",
    "test_dataset = datasets.MNIST('./data', train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "484be2ac-c5b9-4a1c-9215-2a9a8c7097c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./data\n",
       "    Split: Train"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce997c6-77b2-4129-b0d9-0bfd3aa52b69",
   "metadata": {},
   "source": [
    "### Mini-batch SGD will be our training approach\n",
    "The `DataLoader` class in `pytorch` conveniently handles the the stochastic, mini-batch sourcing of our training. Note;\n",
    "\n",
    "The choice of batch size is a trade-off:\n",
    "\n",
    "- Larger batches provide a more accurate estimate of the gradient but require more memory and can lead to a less exploratory update path, potentially getting stuck in shallow local minima.\n",
    "- Smaller batches offer more update opportunities and better generalization at the cost of increased noise in the updates and potentially longer training times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ebeab39-0893-438c-a227-8ba363dfda96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 2^6\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22bd44b-8dde-467d-8790-db64579d6ca1",
   "metadata": {},
   "source": [
    "## Re-introducing you to typical NN's\n",
    "\n",
    "### `pytorch`\n",
    "The SOTA ML framework today that is built upon on the early lua implementation of [the torch library](https://en.wikipedia.org/wiki/Torch_(machine_learning)). Came in during 2017 and has unequivocally been the top tool for ML research.\n",
    "\n",
    "`pytorch` comes with a comprehensive `nn` package that handles common activation functions and architectures for any NN's.\n",
    "\n",
    "#### Input Layer\n",
    "Considering the 28x28 pixel, where each pixel is grayscale restricted, intensity can be presented via a value between 0 and 255, where 0 represents black and 255 represents white."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e25ffdfb-7505-482b-9f21-7a22fee4d77f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
